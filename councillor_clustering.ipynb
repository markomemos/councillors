{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61bc303-8ec3-4ad9-a5b1-32ccf3c8b0c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Municipal Open Data to Analyze Councillor Voting Patterns\n",
    "### by Mark M<sup>c</sup>Carthy\n",
    "\n",
    "I find myself in a position where my previous work is proprietary to my former employers, and I don't have much of a publicly available portfolio. To fix this, I wanted to find a sizeable public dataset then perform a non-trivial analysis, and to have fun doing it! In this project I used attendance data from the city councils of Toronto to study councillor absenteeism, and you can read about this in [another notebook](). I later wrote a clustering algorithm to group councillors based their voting records, developed it with Toronto data and evaluated it with Calgary data, and you can see the analysis below. \n",
    "\n",
    "The clustering algorithm was a heirarchichal agglomerative weight-based non-euclidean clustering using the sum of the variances in the clusters as the objective function to be minimized. A complex imputation was also performed on sparse, non-euclidean data with highly non-uniform uncertainties, with the intent of optimizing the result of the clustering to come.\n",
    "\n",
    "Many municipalities have open source data available on a very wide number of topics. For example, in Toronto you can find data on everything from all public city-provided wifi, to red-light camera locations, to restaurant food inspection results. Here are the data for [Toronto](https://open.toronto.ca/) and [Calgary](https://data.calgary.ca/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25fa29-6477-4702-aaef-981eb5578c15",
   "metadata": {},
   "source": [
    "To get started, let's get all the modules and packages we'll need to analyze the data. Since the datasets were likely to not be overly large, I used Pandas throughout this project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d047bd0-0472-4cbd-aa8a-a70f6a2cea42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version = 2.0.3\n",
      "numpy version = 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print('pandas version = '+pd.__version__)\n",
    "print('numpy version = '+np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2efc52-6604-48f4-a602-2706ef1c9c09",
   "metadata": {},
   "source": [
    "Next we need to grab all the data. To keep it simple, I downloaded it all to a local directory and concatenated the different datafiles into one big table. The column names changed at some point to be more human-readable, from names like `SessionType` to `Session Type`,  but thankfully this was the only way in which they changed, so I could just force a consistent column name upon ingestion. There's a column `_id` which is clearly meant as an index which counts out the entries in each data set, but since I was concatenating different data sets together, I was just going to have to use my own index and ignore this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be81f7fb-5922-4a48-a155-9e5a468f9dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to grab attendance data for Toronto\n",
    "dir_name = '/Users/markmccarthy/Documents/coding_fun/toronto_dsci/councillors/'\n",
    "short_filenames_attendance = [\n",
    "    'councillors-meeting-attendance-2022-2026.csv',\n",
    "    'councillors-meeting-attendance-2018-2022.csv',\n",
    "    'councillors-meeting-attendance-2014-2018.csv',\n",
    "    'councillors-meeting-attendance-2010-2014.csv',\n",
    "    'councillors-meeting-attendance-2006-2010.csv'\n",
    "]\n",
    "filenames_attendance = [dir_name + f for f in short_filenames_attendance] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e431ce8-9372-4d2e-b5b2-7abc16de8b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The columns are consistent but the names are not. Let's fix that\n",
    "col_names = ['_id', 'Term', 'First Name', 'Last Name', 'Committee', 'MTG #', 'Session Date', 'Session Type', 'Session Start-End Time', 'Present']\n",
    "\n",
    "raw_attendance_df = pd.concat((pd.read_csv(f, header=0, names=col_names) for f in filenames_attendance), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489313a-0f34-4d71-888b-b7916dfe52d6",
   "metadata": {},
   "source": [
    "A quick look at the data to get a feel for what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9f19bc-b3fe-4fe5-bd89-a4567625d857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Term</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Committee</th>\n",
       "      <th>MTG #</th>\n",
       "      <th>Session Date</th>\n",
       "      <th>Session Type</th>\n",
       "      <th>Session Start-End Time</th>\n",
       "      <th>Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-2026</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Fletcher</td>\n",
       "      <td>CreateTO</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>Morning</td>\n",
       "      <td>09:31AM - 09:50AM</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-2026</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Ainslie</td>\n",
       "      <td>City Council</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>14:03PM - 15:25PM</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-2026</td>\n",
       "      <td>Brad</td>\n",
       "      <td>Bradford</td>\n",
       "      <td>City Council</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>14:03PM - 15:25PM</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-2026</td>\n",
       "      <td>Alejandra</td>\n",
       "      <td>Bravo</td>\n",
       "      <td>City Council</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>14:03PM - 15:25PM</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-2026</td>\n",
       "      <td>Jon</td>\n",
       "      <td>Burnside</td>\n",
       "      <td>City Council</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>14:03PM - 15:25PM</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id       Term First Name Last Name     Committee  MTG # Session Date  \\\n",
       "0    1  2022-2026      Paula  Fletcher      CreateTO      1   2022-11-18   \n",
       "1    2  2022-2026       Paul   Ainslie  City Council      1   2022-11-23   \n",
       "2    3  2022-2026       Brad  Bradford  City Council      1   2022-11-23   \n",
       "3    4  2022-2026  Alejandra     Bravo  City Council      1   2022-11-23   \n",
       "4    5  2022-2026        Jon  Burnside  City Council      1   2022-11-23   \n",
       "\n",
       "  Session Type Session Start-End Time Present  \n",
       "0      Morning      09:31AM - 09:50AM       N  \n",
       "1    Afternoon      14:03PM - 15:25PM       Y  \n",
       "2    Afternoon      14:03PM - 15:25PM       Y  \n",
       "3    Afternoon      14:03PM - 15:25PM       Y  \n",
       "4    Afternoon      14:03PM - 15:25PM       Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da14563d-2666-47d2-8bd1-b1a287f3d09d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Committee</th>\n",
       "      <th>Session Date</th>\n",
       "      <th>Session Type</th>\n",
       "      <th>Session Start-End Time</th>\n",
       "      <th>Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "      <td>86530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>79</td>\n",
       "      <td>124</td>\n",
       "      <td>2178</td>\n",
       "      <td>3</td>\n",
       "      <td>5787</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2010-2014</td>\n",
       "      <td>John</td>\n",
       "      <td>Crawford</td>\n",
       "      <td>City Council</td>\n",
       "      <td>2011-06-15</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>09:40AM - 12:30PM</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>31894</td>\n",
       "      <td>4878</td>\n",
       "      <td>2677</td>\n",
       "      <td>53409</td>\n",
       "      <td>495</td>\n",
       "      <td>38702</td>\n",
       "      <td>785</td>\n",
       "      <td>74956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term First Name Last Name     Committee Session Date  \\\n",
       "count       86530      86530     86530         86530        86530   \n",
       "unique          5         68        79           124         2178   \n",
       "top     2010-2014       John  Crawford  City Council   2011-06-15   \n",
       "freq        31894       4878      2677         53409          495   \n",
       "\n",
       "       Session Type Session Start-End Time Present  \n",
       "count         86530                  86530   86530  \n",
       "unique            3                   5787       2  \n",
       "top       Afternoon      09:40AM - 12:30PM       Y  \n",
       "freq          38702                    785   74956  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_attendance_df.describe(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8da2ef-3e40-4918-a295-9a2e42917dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                       False\n",
       "Term                      False\n",
       "First Name                False\n",
       "Last Name                 False\n",
       "Committee                 False\n",
       "MTG #                     False\n",
       "Session Date              False\n",
       "Session Type              False\n",
       "Session Start-End Time    False\n",
       "Present                   False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any missing data\n",
    "raw_attendance_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67e392-a282-4db7-9207-7a0bc678a396",
   "metadata": {},
   "source": [
    "Clearly each row is the attendance record for one councillor at one meeting. Mayors seem to be included in these records as if they were just another councillor, so we shouldn't need to provide any special treatment to deal with their data. Now it's time to transform our dataframe in a way that lets us ask meaningful questions of it.<br>\n",
    "First, I grouped the attendance records for each councillor together so we can get how often they attended meetings to which they were eligible to attend. I kept only city council meetings, since they represent 53k of our 86k entries, and some of these committees are going to be difficult to compare to an ordinary council meeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05eaa68e-7b3e-49e4-912a-b3ad4d27d80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>TimesElegibleToAttend</th>\n",
       "      <th>TimesPresent</th>\n",
       "      <th>AttendancePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giorgio Mammoliti</td>\n",
       "      <td>1027</td>\n",
       "      <td>613</td>\n",
       "      <td>59.688413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Ron Moeser</td>\n",
       "      <td>831</td>\n",
       "      <td>515</td>\n",
       "      <td>61.973526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>David Miller</td>\n",
       "      <td>154</td>\n",
       "      <td>100</td>\n",
       "      <td>64.935065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Mike Feldman</td>\n",
       "      <td>154</td>\n",
       "      <td>101</td>\n",
       "      <td>65.584416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Michelle Holland</td>\n",
       "      <td>286</td>\n",
       "      <td>191</td>\n",
       "      <td>66.783217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Doug Holyday</td>\n",
       "      <td>479</td>\n",
       "      <td>471</td>\n",
       "      <td>98.329854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Gord Perks</td>\n",
       "      <td>1340</td>\n",
       "      <td>1322</td>\n",
       "      <td>98.656716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Stephen Holyday</td>\n",
       "      <td>707</td>\n",
       "      <td>704</td>\n",
       "      <td>99.575672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alejandra Bravo</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Olivia Chow</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  TimesElegibleToAttend  TimesPresent  AttendancePercent\n",
       "27  Giorgio Mammoliti                   1027           613          59.688413\n",
       "76         Ron Moeser                    831           515          61.973526\n",
       "18       David Miller                    154           100          64.935065\n",
       "64       Mike Feldman                    154           101          65.584416\n",
       "61   Michelle Holland                    286           191          66.783217\n",
       "..                ...                    ...           ...                ...\n",
       "23       Doug Holyday                    479           471          98.329854\n",
       "30         Gord Perks                   1340          1322          98.656716\n",
       "80    Stephen Holyday                    707           704          99.575672\n",
       "3     Alejandra Bravo                     57            57         100.000000\n",
       "69        Olivia Chow                     11            11         100.000000\n",
       "\n",
       "[83 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a copy to not alter the raw data\n",
    "attend_df = raw_attendance_df.copy()\n",
    "\n",
    "# Remove committee meetings, keep only council meetings\n",
    "attend_df = attend_df.loc[attend_df['Committee']=='City Council']\n",
    "\n",
    "# If we're identifying councillors by name, we should put it in one single column for convenience \n",
    "attend_df['Name'] = attend_df['First Name'] + ' ' + attend_df['Last Name']\n",
    "attend_df = attend_df.drop(columns = ['First Name','Last Name'])\n",
    "\n",
    "# Converting 'Y' or 'N' into 1 or 0 is a handy trick to simplify this sort of data\n",
    "attend_df['Present'] = np.where(attend_df['Present']=='Y', 1, 0)\n",
    "attend_df = attend_df.groupby('Name')['Present'].agg(['count', 'sum'])\n",
    "attend_df = attend_df.rename(columns={'count': 'TimesElegibleToAttend', 'sum': 'TimesPresent'}).reset_index()\n",
    "\n",
    "# This is the column we've all been waiting for, how often did they attend meetings?\n",
    "attend_df['AttendancePercent'] = 100 * attend_df['TimesPresent'] / attend_df['TimesElegibleToAttend']\n",
    "\n",
    "display(attend_df.sort_values(by=['AttendancePercent'],ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47bc8d9-f20c-4f8e-af31-363c92c7d473",
   "metadata": {},
   "source": [
    "Our most reliable councillor is recently elected mayor Olivia Chow! ... with only 15 possible meetings so far. To make a more fair comparison of our dataset we should only include those who have been to a reasonable number of meetings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1acabf94-9470-4f44-b877-2d6316424c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      83.000000\n",
       "mean      643.481928\n",
       "std       437.994274\n",
       "min        11.000000\n",
       "25%       154.000000\n",
       "50%       633.000000\n",
       "75%      1027.000000\n",
       "max      1340.000000\n",
       "Name: TimesElegibleToAttend, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(attend_df.TimesElegibleToAttend.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09fc130-6c09-4488-961c-ff53ba146b27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>TimesElegibleToAttend</th>\n",
       "      <th>TimesPresent</th>\n",
       "      <th>AttendancePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giorgio Mammoliti</td>\n",
       "      <td>1027</td>\n",
       "      <td>613</td>\n",
       "      <td>59.688413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Ron Moeser</td>\n",
       "      <td>831</td>\n",
       "      <td>515</td>\n",
       "      <td>61.973526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Michelle Holland</td>\n",
       "      <td>286</td>\n",
       "      <td>191</td>\n",
       "      <td>66.783217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Rob Ford</td>\n",
       "      <td>751</td>\n",
       "      <td>527</td>\n",
       "      <td>70.173103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Jaye Robinson</td>\n",
       "      <td>1186</td>\n",
       "      <td>900</td>\n",
       "      <td>75.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Sarah Doucette</td>\n",
       "      <td>873</td>\n",
       "      <td>846</td>\n",
       "      <td>96.907216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Frances Nunziata</td>\n",
       "      <td>1340</td>\n",
       "      <td>1304</td>\n",
       "      <td>97.313433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Doug Holyday</td>\n",
       "      <td>479</td>\n",
       "      <td>471</td>\n",
       "      <td>98.329854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Gord Perks</td>\n",
       "      <td>1340</td>\n",
       "      <td>1322</td>\n",
       "      <td>98.656716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Stephen Holyday</td>\n",
       "      <td>707</td>\n",
       "      <td>704</td>\n",
       "      <td>99.575672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  TimesElegibleToAttend  TimesPresent  AttendancePercent\n",
       "27  Giorgio Mammoliti                   1027           613          59.688413\n",
       "76         Ron Moeser                    831           515          61.973526\n",
       "61   Michelle Holland                    286           191          66.783217\n",
       "75           Rob Ford                    751           527          70.173103\n",
       "35      Jaye Robinson                   1186           900          75.885329\n",
       "..                ...                    ...           ...                ...\n",
       "78     Sarah Doucette                    873           846          96.907216\n",
       "24   Frances Nunziata                   1340          1304          97.313433\n",
       "23       Doug Holyday                    479           471          98.329854\n",
       "30         Gord Perks                   1340          1322          98.656716\n",
       "80    Stephen Holyday                    707           704          99.575672\n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    61.000000\n",
       "mean     87.138054\n",
       "std       8.429293\n",
       "min      59.688413\n",
       "25%      83.242401\n",
       "50%      88.467615\n",
       "75%      93.799823\n",
       "max      99.575672\n",
       "Name: AttendancePercent, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can use the 25th percentile as a cutoff, because it's reasonably large and leaves us with plenty of remaining councillors\n",
    "attend_df = attend_df.loc[attend_df['TimesElegibleToAttend']>154].sort_values(by=['AttendancePercent'],ascending=True)\n",
    "display(attend_df)\n",
    "display(attend_df.AttendancePercent.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef77b9-0537-4b4c-a6c3-448ad965cb4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "After removing the low sample size councillors, we arrive at an average (mean) attendance of ~87%. Some councillors have attended almost every meeting they were able to (looking at you, Stephen Holyday!), whereas some are missing a quarter or more! Our anecdote about Rob Ford seems to hold up, only 3 councillors attended fewer meetings. Some quick googling helps to exonerate one of them, Ron Moeser, who suffered from cancer and eventually succumbed while in office.<br>\n",
    "\n",
    "The attendance data doesn't offer us too many more insights, but it does give us one more tidbit when we group by `Session Type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77862a4a-581c-4b22-9c50-a9208ed24759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session Type</th>\n",
       "      <th>TimesElegibleToAttend</th>\n",
       "      <th>TimesPresent</th>\n",
       "      <th>AttendancePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evening</td>\n",
       "      <td>9441</td>\n",
       "      <td>7687</td>\n",
       "      <td>81.421460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afternoon</td>\n",
       "      <td>24806</td>\n",
       "      <td>21431</td>\n",
       "      <td>86.394421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morning</td>\n",
       "      <td>19162</td>\n",
       "      <td>17138</td>\n",
       "      <td>89.437428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Session Type  TimesElegibleToAttend  TimesPresent  AttendancePercent\n",
       "1      Evening                   9441          7687          81.421460\n",
       "0    Afternoon                  24806         21431          86.394421\n",
       "2      Morning                  19162         17138          89.437428"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a copy to not alter the raw data\n",
    "attend_df2 = raw_attendance_df.copy()\n",
    "\n",
    "# Remove committee meetings, keep only council meetings\n",
    "attend_df2 = attend_df2.loc[attend_df2['Committee']=='City Council']\n",
    "\n",
    "# Converting 'Y' or 'N' into 1 or 0 is a handy trick to simplify this sort of data\n",
    "attend_df2['Present'] = np.where(attend_df2['Present']=='Y', 1, 0)\n",
    "attend_df2 = attend_df2.groupby('Session Type')['Present'].agg(['count', 'sum'])\n",
    "attend_df2 = attend_df2.rename(columns={'count': 'TimesElegibleToAttend', 'sum': 'TimesPresent'}).reset_index()\n",
    "\n",
    "# This is the column we've all been waiting for, how often did they attend meetings?\n",
    "attend_df2['AttendancePercent'] = 100 * attend_df2['TimesPresent'] / attend_df2['TimesElegibleToAttend']\n",
    "\n",
    "display(attend_df2.sort_values(by=['AttendancePercent'],ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f907f-8bda-470d-8b4c-7d91db119f25",
   "metadata": {},
   "source": [
    "Councillors are early-birds, not night-owls!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999aa4d-322d-4ea3-9694-4978b641e460",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Councillor clustering based on voting behaviour\n",
    "\n",
    "Now that we've grown comfortable with the open data, let's see if we can do something more sophisticated. If we look at voting records rather than just attendance records, we can try to group councillors together based on how similar they are from each other. To see if we've done a good job clustering, I built everything that follows using the Toronto council voting data. When I was satisfied that the results looked reasonable, I tested to see if it worked using the Calgary council voting data.<br>\n",
    "\n",
    "Unfortunately we see that there is a substantial difference between the two datasets. Toronto's voting records go back to 2006, whereas Calgary's only go back to 2017. To go back further with Calgary's data, it looks like I'd have to do some pdf scraping of the meeting minutes. Since this is supposed to be a ***fun*** project, I'll just let it be. If I were to do this again, I'd try to find a better train-validation-test breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf181cd-ff99-42ce-9621-74c3f882452a",
   "metadata": {},
   "source": [
    "### Data ingestion and cleanup\n",
    "\n",
    "First we need to import our modules and ingest the data. I've decided to duplicate all our dataframes into two sets, a Toronto set and a Calgary set. Had this been a bigger project, I'd love to turn all these into function calls or some kind of sklearn pipelines, but that's beyond the scope for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fa0908-25b0-44a9-af86-fc6a0528031c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version = 2.0.3\n",
      "numpy version = 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print('pandas version = '+pd.__version__)\n",
    "print('numpy version = '+np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df112d05-2800-4d44-a8d6-9a4960d4b0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's label our two datasets, in case I forget my naming scheme\n",
    "city_dict = {0:'Toronto',1:'Calgary'}\n",
    "\n",
    "dir_name = '/Users/markmccarthy/Documents/coding_fun/toronto_dsci/councillors/'\n",
    "filenames = {}\n",
    "\n",
    "# Toronto data \n",
    "short_filenames = [\n",
    "    'member-voting-record-2022-2026.csv',\n",
    "    'member-voting-record-2018-2022.csv',\n",
    "    'member-voting-record-2014-2018.csv',\n",
    "    'member-voting-record-2010-2014.csv',\n",
    "    'member-voting-record-2006-2010.csv'\n",
    "]\n",
    "filenames[0] = [dir_name + f for f in short_filenames] \n",
    "\n",
    "#Calgary data\n",
    "filenames[1] = [dir_name + 'Council_and_Committee_Votes_20231024.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed5ca9-ce46-40dc-ac9a-32856c3e257a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_voting_df = {}\n",
    "\n",
    "# Toronto\n",
    "raw_voting_df[0] = pd.concat((pd.read_csv(f, index_col = '_id') for f in filenames[0]), ignore_index=True)\n",
    "\n",
    "#Calgary\n",
    "raw_voting_df[1] = pd.read_csv(filenames[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510132a6-b97b-4ff6-b4a6-73548c21272b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a sense of what the data looks like\n",
    "for city_id, df in raw_voting_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    display(df.head())\n",
    "    \n",
    "for city_id, df in raw_voting_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a0110-e7c5-43a0-9845-7f3a09fb9a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicated data\n",
    "for city_id, df in raw_voting_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    print(f'Number of duplicated rows = {df[df.duplicated()].shape[0]}')\n",
    "    print('') # could use more whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957416e-9b43-44b2-b39f-af27df4bde56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "for city_id, df in raw_voting_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    display(df.isnull().sum())\n",
    "    print('') # could use more whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bfc7f-9470-4cd0-87a4-859ffc31f6f0",
   "metadata": {},
   "source": [
    "We can see that the Toronto and Calgary data have a lot in common. Both have varied `Committee` or `MeetingType` including council votes and committee votes, and each row represents each councillor at each vote. We can already see some features of the data, for example, the Toronto `Date/Time` column is mostly empty, so we should be careful if ever using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9676bd-313a-4e6d-8715-6b9f976762f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for city_id, df in raw_voting_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    # it's quite convenient that the column name 'Vote' appears in both datasets!\n",
    "    display(df.Vote.value_counts())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c1d37-12c5-4757-9394-7ef60ca0b0df",
   "metadata": {},
   "source": [
    "One major difference between the datasets is apparent here... In Toronto absent councillors are always given a row in the data where they `Vote` `Absent`, whereas the absent councillors in Calgary are omitted from the table entirely, with rare exceptions.<br><br>\n",
    "This feature lets us quickly redo our attendance analysis in Toronto quite easily, but on a per-vote rather than per-meeting basis! To do so in Calgary, however, would require ingesting some new data that tells us who was a councillor and when. We could then impute the `Absent`s ourselves, but there may be many situations where councillors are not on a committee or otherwise not eligible to vote, which could cause problems.<br><br>\n",
    "So, for now, let's try to quickly redo our attendance analysis on a per-vote basis and see if we get similar results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c86a9-171b-419b-be22-c1956025577c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check which councillors missed votes, Toronto only\n",
    "voting_attend_df = raw_voting_df[0].copy()\n",
    "\n",
    "# Get rid of committee votes \n",
    "voting_attend_df = voting_attend_df.loc[voting_attend_df['Committee'] == 'City Council']\n",
    "\n",
    "# Group the names into one column (this isn't strictly necessary)\n",
    "voting_attend_df['Name'] = voting_attend_df['First Name'] + ' ' + voting_attend_df['Last Name']\n",
    "voting_attend_df = voting_attend_df.drop(columns = ['First Name','Last Name'])\n",
    "\n",
    "# Group by councillor and get the percentage of absent votes\n",
    "voting_attend_df['Absent'] = np.where(voting_attend_df['Vote']=='Absent', 1, 0)\n",
    "voting_attend_df = voting_attend_df.groupby('Name')['Absent'].agg(['count', 'sum'])\n",
    "voting_attend_df['AbsentPercent'] = 100 * voting_attend_df['sum'] / voting_attend_df['count']\n",
    "voting_attend_df = voting_attend_df.rename(columns={'count': 'EligibleVotes', 'sum': 'TimesAbsent'}).reset_index()\n",
    "\n",
    "# Remove councillors with a low vote count\n",
    "# There are probably better choices for a cut-off, but the 25th percentile is at least consistent with the previous analysis\n",
    "display(voting_attend_df.EligibleVotes.describe())\n",
    "voting_attend_df = voting_attend_df.loc[voting_attend_df['EligibleVotes'] > voting_attend_df.EligibleVotes.quantile(0.25)]\n",
    "\n",
    "# Display results\n",
    "display(voting_attend_df.sort_values(by=['AbsentPercent'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e5e4d-86e6-43c6-a84b-d384a917da7c",
   "metadata": {},
   "source": [
    "Looks good, a lot of the same names show up as before! The per-meeting numbers are probably more meaningful than the per-vote numbers because meeting attendance is more likely to be independent data points, but it's nice to see some relative agreement.<br>\n",
    "<br>\n",
    "Next, we can look at one of the remaining unexamined columns in the Toronto data, the `Motion Type`. I can imagine that some motions are controversial, some motions are skippable, and some are good reflections of a candidates viewpoints. Let's see how it breaks down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45179fc-7e0c-43c5-bc20-7d9f96ba5715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check which Motion Types are controversial and/or commonly skipped\n",
    "voting_motion_type_df = raw_voting_df[0].copy()\n",
    "\n",
    "# Get the total votes of each response grouped by Motion Type\n",
    "voting_motion_type_df = voting_motion_type_df.groupby('Motion Type').apply(lambda x: pd.Series(dict(\n",
    "    yes=(x.Vote=='Yes').sum(),\n",
    "    no=(x.Vote=='No').sum(),\n",
    "    absent=(x.Vote=='Absent').sum()\n",
    ")))\n",
    "\n",
    "# Now let's calculate the secondary numbers\n",
    "voting_motion_type_df['Total'] = voting_motion_type_df['yes'] + voting_motion_type_df['no'] + voting_motion_type_df['absent']\n",
    "\n",
    "voting_motion_type_df['Yes %']    = 100 * voting_motion_type_df['yes'] / voting_motion_type_df['Total']\n",
    "voting_motion_type_df['No %']     = 100 * voting_motion_type_df['no'] / voting_motion_type_df['Total']\n",
    "voting_motion_type_df['Absent %'] = 100 * voting_motion_type_df['absent'] / voting_motion_type_df['Total']\n",
    "\n",
    "# Sort by no%... how controversial are these motion types?\n",
    "display(voting_motion_type_df[['Total','Yes %','No %','Absent %']].sort_values(by='No %', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8251cb-23d9-44c4-bdbf-33a2f3ba0a76",
   "metadata": {},
   "source": [
    "Here we reach a difficult decision... should any motion types be removed because they do not actually reflect councillor voting patterns, which is what we want to observe? You could argue that `Adopt Minutes` and `Introduce Report` are so agreeable that they offer little information about our voters, but it's unclear how to define when a motion is meaningless compared to insightful. Even votes on banal-sounding issues like `Recess` could contain information on the political leanings of councillors, so it's tough to justify removing any of these. Perhaps more importantly, it would be far more difficult to remove similar motions from the Calgary data since this information is obscured within the `Resolution` column, and treating the datasets differently would undermine comparisons between the two. \n",
    "\n",
    "In light of this, I've chosen to keep all of our motion types. It's something we can keep in mind if we ever revisit this analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a875b79-933a-4ba9-908b-304d6bc9dc8d",
   "metadata": {},
   "source": [
    "Let's now take a closer look at the Calgary data and see if there is any cleanup required. First we notice some councillor naming inconsistencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e195019-ade0-4ba2-acc4-89f3e763fef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(raw_voting_df[1].loc[raw_voting_df[1]['Voter'].str.contains('Gondek')].Voter.value_counts())\n",
    "print()\n",
    "display(raw_voting_df[1].loc[raw_voting_df[1]['Voter'].str.contains('Pootmans')].Voter.value_counts())\n",
    "print()\n",
    "display(raw_voting_df[1].loc[raw_voting_df[1]['Voter'].str.contains('Nenshi')].Voter.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9eb1fb-205c-458c-bf5b-e8b24170c095",
   "metadata": {},
   "source": [
    "The time period of our data spans when Jyoti Gondek was a councillor and when she became mayor. However, she remains just one person, not two, so we need to merge her entries. We should probably also change Naheed Nenshi's name to be consistent with all the councillors. Lastly, we ought to fix those two times that John (Richard) Pootmans' name was entered differently.\n",
    "\n",
    "One interesting feature of Calgary's municipal government is that there are some committees where private citizens are appointed to the committee and will vote along with some councillors. Since we're not including committee votes anyway, this won't be an issue for our clustering later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37261766-df18-4e9f-b86c-fb19823800ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_councillors = raw_voting_df[1].loc[raw_voting_df[1]['MeetingType'].str.contains('Council')].Voter.unique()\n",
    "display(raw_voting_df[1].loc[raw_voting_df[1]['Voter'].isin(list_of_councillors)==False].MeetingType.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4384d3b-47d7-4b6c-a064-6ee32bb72a2b",
   "metadata": {},
   "source": [
    "Lastly, let's look at those rare `Vote`s that are `Absent` and make sure nothing strange is happening there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d7ea2-25df-4643-a9e5-4bb84688cb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(raw_voting_df[1].loc[raw_voting_df[1].Vote =='Absent'].Voter.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea1d1e-6ec5-4f21-a309-fbb3a1ba7b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait, who is '. .'?\n",
    "display(raw_voting_df[1].loc[raw_voting_df[1].Voter =='. .'].Vote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8890da5-7049-4df6-9db7-996015d75d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A 'yes' vote was unattributed???\n",
    "display(raw_voting_df[1].loc[(raw_voting_df[1].Voter == '. .') & (raw_voting_df[1].Vote == 'Yes')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277c489-76c2-4e96-a7dc-ee18a52ef119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When did this vote happen? How did it happen?\n",
    "temp_df = raw_voting_df[1].loc[(raw_voting_df[1].Voter == '. .') & (raw_voting_df[1].Vote == 'Yes')].drop(columns=['Voter','Vote'])\n",
    "col_list = temp_df.columns.to_list()\n",
    "\n",
    "display(pd.merge(left = temp_df, right = raw_voting_df[1], on = col_list, how='left'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50b3a3-0d62-41d0-a9cd-c8f5f5938792",
   "metadata": {},
   "source": [
    "I can understand that someone keeping minutes of council meetings might be a bit lazy when entering an `Absent` vote and input a name like `. .`, but inputting a `Yes` vote seems outlandish! Cross-referencing the list of councillors in the meeting and the councillors at the time show that Ray Jones from Ward 10 is missing. So clearly `. .` is Ray Jones, right? Well, Ray Jones resigned from council in October 2020 citing health issues and no by-election was held, so the seat was vacant until the next election in October 2021. This looks like an actual mistake where a vacant seat was recorded as voting `Yes`, but thankfully the result of the vote was not altered by the mistake!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78366652-30a1-4324-8a46-b0eb2e86934e",
   "metadata": {},
   "source": [
    "Now that we understand the peculiarities of our data, we can finally clean it up and get ready to cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa25bcc-f8c0-4a6b-91b4-c7c24bb1e476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleaning raw data:\n",
    "clus_vote_df = {}\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Toronto\n",
    "clus_vote_df[0] = raw_voting_df[0].copy()\n",
    "\n",
    "# The Date/Time column is mostly empty\n",
    "clus_vote_df[0] = clus_vote_df[0].drop(columns='Date/Time')\n",
    "\n",
    "# Only council meetings\n",
    "clus_vote_df[0] = clus_vote_df[0].loc[clus_vote_df[0].Committee == 'City Council']\n",
    "\n",
    "# Consolidating the names into one column makes life easier\n",
    "clus_vote_df[0]['Name'] = clus_vote_df[0]['First Name'] + ' ' + clus_vote_df[0]['Last Name']\n",
    "clus_vote_df[0] = clus_vote_df[0].drop(columns = ['First Name','Last Name'])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Calgary\n",
    "clus_vote_df[1] = raw_voting_df[1].copy()\n",
    "\n",
    "# Only council meetings\n",
    "clus_vote_df[1] = clus_vote_df[1].loc[clus_vote_df[1]['MeetingType'].str.contains('Council')]\n",
    "\n",
    "# Standardize councillor names\n",
    "clus_vote_df[1] = clus_vote_df[1].replace({'Mayor J. Gondek':'Jyoti Gondek',\n",
    "                                     'Mayor N. Nenshi':'Naheed Nenshi',\n",
    "                                     'Richard Pootmans':'John (Richard) Pootmans'})\n",
    "\n",
    "# Rename the Voter column to make consistent with the Toronto data\n",
    "clus_vote_df[1] = clus_vote_df[1].rename(columns={'Voter': 'Name'})\n",
    "\n",
    "# Remove that time an empty chair voted \"Yes\"\n",
    "clus_vote_df[1] = clus_vote_df[1][clus_vote_df[1].Name != '. .']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbf7e5-ccba-4036-a503-eb6a979aef2f",
   "metadata": {},
   "source": [
    "It's possible there were multiple identical votes on the same day, but there's no good way to tell if they are data input problems or real votes. Even if they are real votes, it's still probably okay to drop since identical votes are  not going to provide much additional insight into our councillors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d53aab-eefd-41cd-bdef-99d03dcdfa3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clus_vote_df[0] = clus_vote_df[0].drop_duplicates()\n",
    "clus_vote_df[1] = clus_vote_df[1].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cadaa8-fc82-4e68-974e-e06eae63c591",
   "metadata": {},
   "source": [
    "Our data has one voter's vote per row. We want to cluster councillors based on how often they vote together. This means we want our data to represent the distribution of voters responses each time they voted. We first need to identify each individual resolution that the voters were asked to vote upon, add that column back into our original dataframe, then perform a pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423404f-6a59-4a43-8d06-4d295d6fa775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_df = {}\n",
    "for city_id, df in clus_vote_df.items():\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "\n",
    "    # All the information about the resolution, not the voter\n",
    "    col_list = df.columns.drop(['Name','Vote']).to_list()\n",
    "\n",
    "    # This maddening line collects all the resolution-based information and assigns a \"vote_id\" to each\n",
    "    temp_df = df.groupby(col_list).size().reset_index().drop(columns=0).reset_index().rename(columns={'index':'vote_id'})\n",
    "    \n",
    "    # We merge this back into the original dataframe so we have a new \"vote_id\" column\n",
    "    merge_df = pd.merge(left = temp_df, right = df, on = col_list, how='left')\n",
    "    \n",
    "    # To get ready to pivot, we only need the name, vote, and vote_id\n",
    "    # We also need there to be no duplicates, but somehow we have ~100 in the Toronto data? This is a bug I haven't resolved\n",
    "    merge_df = merge_df.drop(columns=col_list).drop_duplicates(subset=['vote_id','Name'])\n",
    "\n",
    "    # Finally pivot\n",
    "    pivot_df[city_id] = merge_df.pivot(index='vote_id', columns='Name', values='Vote')\n",
    "    \n",
    "    # And replace absences with NaNs since we're done evaluating attendance\n",
    "    pivot_df[city_id] = pivot_df[city_id].replace('Absent',np.NaN)\n",
    "\n",
    "    display(pivot_df[city_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a2b6b-9768-4a21-81b2-fa3bcd3f97b5",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Now we finally have our data in a format where we can compare how similar two councillors are based upon how frequently they vote together. To do this in a sensible way, we should make a model about how councillors vote.\n",
    "\n",
    "One possible place to start is to assume that each councillor's response on each vote is essentially a Bernoulli trial with some probability $p$ of voting `Yes` and $1-p$ of voting `No`. These probabilities would vary wildly vote to vote, so it's very unreasonable to turn this directly into some sort of Binomial and make generalized statements about voting behaviour.\n",
    "\n",
    "However, when we look at two councillors, there is a different Bernoulli trial occurring... these two councillors will give the same response to a vote with some probability $p$ and different responses with $1-p$. We've seen that some motions are more likely to produce disagreements than others, but over a large enough sample size, it's not unreasonable to say that these probabilities of (dis)agreement are fairly constant between those two councillors. Suddenly, with a few caveats, we can arrive at a Binomial!\n",
    "\n",
    "Specifically, if councillors $A$ and $B$ are both voting in $n_{AB}$ votes, they'll disagree $d_{AB}$ times with a probability of disagreeing $D_{AB}$.\n",
    "$$d_{AB} \\sim \\text{Binomial}(n_{AB},D_{AB})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ef8ea-a215-401e-b5a3-278cd8e2f47c",
   "metadata": {},
   "source": [
    "Before we get ahead of ourselves, what are those caveats?\n",
    "\n",
    "1. $D_{AB}$ is assumed to be independent of vote, which really only holds if each pair of councillors get a similar distribution of motions to vote upon and enough votes to let $D_{AB}$ truly represent some kind of average disagreement probability\n",
    "2. Councillors are assumed to be static and unchanging over the time they spend in office.\n",
    "3. Votes are assumed to be independent of one another. \n",
    "\n",
    "There are more assumptions we could acknowledge, but these three seem the most dubious. If we ever wanted to return to this project and improve our confidence in the result, tackling caveat #1 by controlling the motion types represented in our data would be a good starting point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799b24b-3fec-4632-9bdb-3a46c134ecf5",
   "metadata": {},
   "source": [
    "From our data, $n_{AB}$ and $d_{AB}$ are numbers we can directly compute. $D_{AB}$ however needs to be estimated. We could simply use the maximum likelihood estimator (MLE):\n",
    "\n",
    "$$\\hat{D}_{AB} = \\frac{d_{AB}}{n_{AB}}$$\n",
    "\n",
    "There are two issues with this. Firstly, not all estimates of $D_{AB}$ will be equal since some pairs of councillors will have thousands of votes and some will only have a few (represented by $n_{AB}$), and it would be good to differentiate these situations. Secondly, we will have some pairs of councillors that agreed every (or almost every) time they were able to vote together... this usually happens when $n_{AB}$ is very low, but that doesn't mean we genuinely believe $D_{AB}$ should be zero.\n",
    "\n",
    "A bayesian approach tackles both these problems simultaneously. As is typical with estimating Binomial probabilities, we'll use the Beta distribution as a conjugate prior for $D_{AB}$,\n",
    "\n",
    "$$D_{AB} \\sim \\text{Beta}(\\alpha,\\beta)$$\n",
    "\n",
    "Now we've just pushed our problem onto estimating $\\alpha$ and $\\beta$. Bayesian inference gives us a means for updating our estimates for these two (hyper/meta)parameters as we acquire more data, specifically:\n",
    "\n",
    "$$\\alpha' = \\alpha + d_{AB}$$\n",
    "$$\\beta' = \\beta + (n_{AB}-d_{AB})$$\n",
    "\n",
    "We need to know the values of $\\alpha$ and $\\beta$ before we have any data, or our prior, and for simplicity's sake we can choose the uniform prior of $\\alpha = \\beta = 1$. Using a different prior (like Jeffrey's) is not a bad idea, but hopefully with large enough $n_{AB}$ this shouldn't matter. Our final estimator becomes:\n",
    "\n",
    "$$\\hat{D}_{AB} = \\frac{\\alpha'}{\\alpha'+\\beta'}$$\n",
    "\n",
    "$$= \\frac{d_{AB} + 1}{n_{AB} + 2}$$\n",
    "\n",
    "$$\\sigma^2_{\\hat{D}_{AB}} = \\frac{(d_{AB} + 1)(n_{AB}-d_{AB}+1)}{(n_{AB} + 2)^2(n_{AB}+3)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f23b0-9667-40b2-8bf1-dfb7310bb34a",
   "metadata": {},
   "source": [
    "At long last, we have the elements we need for a dissimilarity matrix for our councillors. We can use $D_{ij}$ as our dissimilarity value and $w_{ij}=\\frac{1}{\\sigma_{ij}^2}$ as a weighting.\n",
    "\n",
    "One final element to consider is that when $n_{AB}$ is very small, we run into a number of assumptions failing, so we'll be sure to remove elements when this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a608b7e-019f-452e-9bd2-0a7370ba3885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first get matrix for d_ij and n_ij, then use those to compute D_ij and w_ij\n",
    "d_df = {}\n",
    "n_df = {}\n",
    "\n",
    "n_cutoff = 100\n",
    "\n",
    "for city_id, df in pivot_df.items():\n",
    "    # Unfortunately I can't think of a clean way to do this in pandas without iterating through the columns, which is slower.\n",
    "    # At least we can resolve only half the matrix and rely on symmetry to get the other half!\n",
    "    all_cols = df.columns\n",
    "    rem_cols = df.columns\n",
    "    d_df[city_id] = pd.DataFrame(columns = all_cols, index = all_cols, dtype = int)\n",
    "    for col1 in all_cols:\n",
    "        rem_cols = rem_cols.drop(col1)\n",
    "        # Doing this should ensure we have every unordered combination of col1, col2\n",
    "        for col2 in rem_cols:\n",
    "            # Disagreements are when both voters voted (not null) but voted differently\n",
    "            d_df[city_id].at[col1,col2] = len(df.loc[df[col1].notnull() & df[col2].notnull() & (df[col1] != df[col2])])\n",
    "    \n",
    "    # reset rem_cols so we can loop through again \n",
    "    rem_cols = df.columns\n",
    "    n_df[city_id] = pd.DataFrame(columns = all_cols, index = all_cols, dtype = int)\n",
    "    for col1 in all_cols:\n",
    "        rem_cols = rem_cols.drop(col1)\n",
    "        for col2 in rem_cols:\n",
    "            val = len(df.loc[df[col1].notnull() & df[col2].notnull()])\n",
    "            # remove entries where n is less than the cutoff\n",
    "            n_df[city_id].at[col1,col2] = val if val >= n_cutoff else np.NaN\n",
    "\n",
    "    n_df[city_id] = n_df[city_id].replace(0,np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54646e3-56e4-4a10-8023-495dff4800f0",
   "metadata": {},
   "source": [
    "Since $D_{ij}$ and $w_{ij}$ can be performed element-wise, they are much simpler and easier to compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a13eb6-d131-42e8-880d-824f9275b0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_df = {}\n",
    "w_df = {}\n",
    "\n",
    "for city_id in [0,1]:\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    \n",
    "    D_df[city_id] = (d_df[city_id]+1)/(n_df[city_id]+2)\n",
    "    w_df[city_id] = 1/((d_df[city_id]+1)*(n_df[city_id]-d_df[city_id]+1)/((n_df[city_id]+2)**2*(n_df[city_id]+3)))\n",
    "    \n",
    "    display(D_df[city_id])\n",
    "    display(w_df[city_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11fe8c-a520-46eb-ba3d-e1da91657cad",
   "metadata": {},
   "source": [
    "Despite that these matrices are symmetric, so we can fill in the bottom-left values with the top-right values, these matrices are pretty sparse! This is because plenty of councillors never served with other councillors on this list, and so we can't directly state how similar or dissimilar they are. \n",
    "\n",
    "I did my fair share of googling clustering algorithms for sparse boolean non-euclidean data with weights, but found nothing satisfying. Imputting our missing values with something meaningful would be helpful, but that's not intuitive.\n",
    "\n",
    "Still, this is supposed to be a *fun* project, and to me, inventing a clustering algorithm from scratch actually does seem ***fun***!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d500d-b72d-410d-9870-133e617a3c8b",
   "metadata": {},
   "source": [
    "To make the clustering easier, it would be really nice if we could fill in the missing entries in our dissimilarity matrix. If we know $D_{AB}$ and $D_{BC}$ but not $D_{AC}$, is there anything we can say about it anyway? And can we bias our results so that our clustering will prefer to group councillors who coexisted simultaneously over their hypothetical time-travelling colleagues?\n",
    "\n",
    "One idea that comes to mind comes from looking at the probabilities of agreement, or $A_{ij} = 1-D{ij}$. Specifically the probability of A and C agreeing would be connected to the probability of A agreeing with B, and B agreeing with C. In math:\n",
    "\n",
    "$$A_{AC}=A_{AB}A_{BC}$$\n",
    "$$D_{AC}=1-((1-D_{AB})(1-D_{BC})$$\n",
    "\n",
    "With the weights being the inverse variances, we should probably find $w_{AC}$ too while we're at it. Using the *actually not very great* assumption that these are gaussian variances, we can propagate our uncertainty as follows:\n",
    "\n",
    "$$\\sigma_{AC}^2 = D_{AC}^2 \\left[\\frac{\\sigma_{AB}^2}{(1-D_{AB})^2}+\\frac{\\sigma_{AC}^2}{(1-D_{AC})^2}\\right]$$\n",
    "\n",
    "Of course since these are probabilities, $A,D\\in[0,1]$, which means that the following relations are true:\n",
    "\n",
    "$$A_{AC} < A_{AB}, A_{BC}$$\n",
    "$$D_{AC} > D_{AB}, D_{BC}$$\n",
    "\n",
    "But that's actually a good thing! If $D_{ij}$ is something to minimize in a cluster, then we want councillors to group with their co-existing colleagues first!\n",
    "\n",
    "One final wrinkle arrives in the very likely event that councillors A and Z never vote together, but there are many B,C,D,... councillors that vote with both A and Z. How do we handle this? Since we already know how to calculate every path for $D_{AZ}$ along with every associated uncertainty, and we know these values are all bigger than their intermediate steps, we can  just find the mean of all these paths, weighted by their individual inverse variances! When taking the variance for this average, we have to remember to grab the weighted variance of the paths, not the weighted variance of the mean.\n",
    "\n",
    "While there's no single correct answer for an imputation like this and there may be simpler options or more rigourous options, this choice seems to balance both in my mind and also winds up giving good results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0cceb-bbb3-4115-8d32-03af3aa289e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write out some functions that we'll use a lot of\n",
    "def weightedVariance(input_vals, sd=False):\n",
    "    '''\n",
    "    vals = [x[0] for x in input_vals]\n",
    "    #if sd:\n",
    "        #weights = [(1/(x[1])**2) for x in input_vals]\n",
    "    #else:\n",
    "    weights = [x[1] for x in input_vals]\n",
    "    mean = weightedMean(input_vals, sd)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    '''\n",
    "    mean = weightedMean(input_vals)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for v,w in input_vals:#zip(vals, weights):\n",
    "        num += w*((v-mean)**2)\n",
    "        denom += w\n",
    "    return num/denom\n",
    "        \n",
    "def weightedMean(input_vals, sd=False):\n",
    "    '''\n",
    "    weights = []\n",
    "    vals = [x[0] for x in input_vals]\n",
    "\n",
    "    #if sd:\n",
    "    #    weights = [(1/(x[1])**2) for x in input_vals]\n",
    "    #else:\n",
    "    weights = [x[1] for x in input_vals]\n",
    "    '''\n",
    "    \n",
    "    num = 0\n",
    "    denom = 0\n",
    "\n",
    "    for v,w in input_vals:#zip(vals, weights):\n",
    "        num += v*w\n",
    "        denom += w\n",
    "        \n",
    "    return num/denom\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a9bb-b316-4093-b6f3-afada32926a5",
   "metadata": {},
   "source": [
    "This next function is probably the biggest user of processing time in this whole project. For lists of councillors any bigger, this would be the first target to be migrated to C or to be rewritten in some other fashion. \n",
    "\n",
    "The idea is to find all the missing values in the table, then use the connections between the councillors to fill in as much as we can. It's possible there are councillors with no other councillors connecting them, but that's okay, once we've filled in our missings once, we are far more likely to have just imputed a value that will help us make a new connection!\n",
    "\n",
    "It's also possible that there is some set of councillors who have never voted with another set of councillors, and that no connecting councillor exist. In that case, this will fail, but so too will our entire clustering process. This seems a very unlikely situation however, so it falls in the category of \"problems that might not actually be problems\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ea356-0e80-4451-aee3-ff7aaffcc14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imputeMissings(clus_df, wclus_df):\n",
    "    # this function alters clus_df and wclus_df!\n",
    "    \n",
    "    i=0\n",
    "    while clus_df.isna().sum().sum() > 0:\n",
    "        print(f'in while loop, i={i}')\n",
    "        i+=1\n",
    "        \n",
    "        # find array locations of all missing elements\n",
    "        nans = np.where(clus_df != clus_df)\n",
    "        print(f'--There are {len(nans[0])} missing elements')\n",
    "        \n",
    "        j=0\n",
    "        #iterate through missings and find where the two unconnected nodes have a node in common\n",
    "        for nans_row, nans_col in zip(nans[0],nans[1]):\n",
    "            if j%100 == 0:\n",
    "                print(f'processed {j} missing elements')\n",
    "            j+=1\n",
    "            if clus_df.iat[nans_row,nans_col] == clus_df.iat[nans_row,nans_col]:\n",
    "                #this can happen because of the symmetrization\n",
    "                continue\n",
    "            nan_row_options = clus_df.iloc[nans_row].dropna().index\n",
    "            nan_col_options = clus_df.iloc[:,nans_col].dropna().index\n",
    "            connections = np.intersect1d(nan_row_options,nan_col_options)\n",
    "                        \n",
    "            # if no nodes in common, move on. Eventually we should get every NaN unless data unclusterable somehow\n",
    "            if len(connections)==0:\n",
    "                continue\n",
    "\n",
    "            # if there are multiple common nodes, we want to loop over them and take an average\n",
    "            # the agreement value for each common node connection is a product of the agreements\n",
    "            vals = []\n",
    "            for c in connections:\n",
    "                d_ab = clus_df.iloc[nans_row][c]\n",
    "                d_bc = clus_df.iloc[:,nans_col][c]\n",
    "                d_ac = 1-(1-d_ab)*(1-d_bc)\n",
    "                w_ab = wclus_df.iloc[nans_row][c]\n",
    "                w_bc = wclus_df.iloc[:,nans_col][c]\n",
    "                w_ac = 1/(d_ac**2*(1/((1-d_ab)**2*w_ab)+1/((1-d_bc)**2*w_bc)))\n",
    "                vals.append((d_ac,w_ac))\n",
    "            val = weightedMean(vals)\n",
    "            # We want the stdev of the distribution, not of the mean\n",
    "            w_val = 1/weightedVariance(vals)\n",
    "            \n",
    "            #fill in our dataframes with the results, don't forget to keep symmetric!\n",
    "            clus_df.iat[nans_row,nans_col] = val\n",
    "            clus_df.iat[nans_col,nans_row] = val\n",
    "            wclus_df.iat[nans_row,nans_col] = w_val\n",
    "            wclus_df.iat[nans_col,nans_row] = w_val    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8e158-32ea-4463-86b3-128ed7b00c02",
   "metadata": {},
   "source": [
    "Now that we've defined our functions, let's actually do the imputation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c6b18-bf5e-41c2-9111-9d1246c7e645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a directory to store the imputed data so we don't have to redo this step too often.\n",
    "os.makedirs('intermediates', exist_ok=True) \n",
    "\n",
    "# We'll keep our original D_df and w_df just in case\n",
    "D_imp_df = {}\n",
    "w_imp_df = {}\n",
    "\n",
    "for city_id in [0,1]:\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "\n",
    "    # For simplicity, we'll assign a number to each councillor\n",
    "    D_imp_df[city_id] = D_df[city_id].copy()\n",
    "    D_imp_df[city_id].index = range(0,len(D_imp_df[city_id].columns))\n",
    "    D_imp_df[city_id].columns = range(0,len(D_imp_df[city_id].columns))\n",
    "    w_imp_df[city_id] = w_df[city_id].copy()\n",
    "    w_imp_df[city_id].index = range(0,len(w_imp_df[city_id].columns))\n",
    "    w_imp_df[city_id].columns = range(0,len(w_imp_df[city_id].columns))\n",
    "\n",
    "\n",
    "    # Make symmetric \n",
    "    rem_cols = D_imp_df[city_id].columns.copy()\n",
    "    for col1 in D_imp_df[city_id].columns:\n",
    "        rem_cols.drop(col1)\n",
    "        for col2 in rem_cols:\n",
    "            D_imp_df[city_id].at[col2,col1] = D_imp_df[city_id].at[col1,col2]\n",
    "            w_imp_df[city_id].at[col2,col1] = w_imp_df[city_id].at[col1,col2]\n",
    "\n",
    "    # Remove councillors with no entries. This happens because of our n_ij > n_cutoff we did earlier\n",
    "    drop_cols = D_imp_df[city_id].columns[D_imp_df[city_id].notna().any() == False]\n",
    "    print(f'The following councillors have been dropped due to low n:{drop_cols.to_list()}')\n",
    "    D_imp_df[city_id] = D_imp_df[city_id].drop(drop_cols, axis = 0).drop(drop_cols, axis = 1)\n",
    "    w_imp_df[city_id] = w_imp_df[city_id].drop(drop_cols, axis = 0).drop(drop_cols, axis = 1)\n",
    "\n",
    "    # Set trace to 0\n",
    "    for col1 in D_imp_df[city_id].columns:\n",
    "        D_imp_df[city_id].at[col1,col1] = 0\n",
    "\n",
    "\n",
    "    imputeMissings(D_imp_df[city_id], w_imp_df[city_id])\n",
    "\n",
    "    display(D_imp_df[city_id].head())\n",
    "    display(w_imp_df[city_id].head())\n",
    "\n",
    "    # Save to file to save time if we have to redo this later\n",
    "    D_imp_df[city_id].to_csv(f'intermediates/D_imp_df{city_id}.csv')\n",
    "    w_imp_df[city_id].to_csv(f'intermediates/w_imp_df{city_id}.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec78c8-29df-4a1d-979d-d3b430931dd8",
   "metadata": {},
   "source": [
    "Next we need some metric evaluate how good a cluster is. Our dissimilarity matrix already gives us a value for how dissimilar a cluster of any 2 councillors are, and if we ditch the idea that the dissimiliarity has anything to do with probability of voting, we can interpret it as a type of distance, and its square as a type of variance.\n",
    "\n",
    "Clustering based on variance is a very common metric (see k-means clustering), but there is a catch with our data. Not actually being a distance, it's non-euclidean, meaning that there's no natural physical incentive for our data points to spread out in the same way that points on a map might be spread. This leads our clustering to favour having one large group with all councillors in one spot, and several tiny one-councillor sized clusters. We can \"fix\" this by optimizing the variance multiplied by the number of points in a cluster, thus biasing our clusters to be equally sized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e48bb-5a9a-48c1-979c-6b0da5dc32d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cluster variance based on councillors in cluster\n",
    "def getClusVar(cols, D_df, w_df):\n",
    "    if len(cols) == 1:\n",
    "        return 0\n",
    "    \n",
    "    num = 0\n",
    "    denom = 0\n",
    "    \n",
    "    rem_cols = cols.copy()\n",
    "    for col1 in cols:\n",
    "        rem_cols.remove(col1)\n",
    "        for col2 in rem_cols:\n",
    "            v = D_df.at[col1,col2]\n",
    "            w = w_df.at[col1,col2]\n",
    "            num += w*v**2\n",
    "            denom += w\n",
    "            \n",
    "    if denom != denom:\n",
    "        return np.NaN\n",
    "    return num/denom\n",
    "\n",
    "# Cluster variance multiplied by size of cluster (via the weights of the councillors in the cluster)\n",
    "def getClusNVar(cols, D_df, w_df):\n",
    "    if len(cols) == 1:\n",
    "        return 0\n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    rem_cols = cols.copy()\n",
    "    for col1 in cols:\n",
    "        rem_cols.remove(col1)\n",
    "        for col2 in rem_cols:\n",
    "            v = D_df.at[col1,col2]\n",
    "            w = w_df.at[col1,col2]\n",
    "            num += w*v**2\n",
    "\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86e064-fdfa-4bd6-ab74-65c605d72d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering based on objective function ~= variance * cluster size\n",
    "\n",
    "# This keeps track of which councillors are in which cluster\n",
    "clusters_id = {}\n",
    "\n",
    "# This keeps track of how the total objective function changes with the number of clusters\n",
    "nClus_to_obj_func = {}\n",
    "\n",
    "# This keeps track of which clusters existed at what points\n",
    "clus_timeline = {}\n",
    "\n",
    "# This keeps track of councillor_id numbers and councillor names\n",
    "id_to_name = {}\n",
    "\n",
    "for city_id in [0,1]:\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "\n",
    "    # We need to know who the councillors are\n",
    "    councillor_list = D_df[city_id].columns.to_list()\n",
    "    id_to_name[city_id] = dict(zip(range(0,len(councillor_list)),councillor_list))\n",
    "\n",
    "    # Clus_df has the rows and columns of clusters, for now each with just a single councillor\n",
    "    # The non-diagonal elements represent how much the to-be-minimized objective function grows with each merging of clusters\n",
    "    clus_df = D_imp_df[city_id]**2\n",
    "    \n",
    "    # Unfortunately we have to set the diagonal to NaN so we can take the min later and not grab the diagonal values\n",
    "    for col1 in clus_df.columns:\n",
    "        clus_df.at[col1,col1]=np.NaN\n",
    "\n",
    "    # Make initial clusters of 1 councillor per cluster\n",
    "    # Clusters_id[city_id] is of the form {cluster_id : ([list of councillor_ids],N*Variance)}\n",
    "    clusters_id[city_id] = {}\n",
    "    for col in clus_df.columns:\n",
    "        clusters_id[city_id].update({col:([col],0)})\n",
    "\n",
    "    # Initialize these before clustering\n",
    "    nClus_to_obj_func[city_id] = {}\n",
    "    clus_timeline[city_id] = {}\n",
    "\n",
    "    # This is the objective function with the current number of clusters\n",
    "    nClus_to_unexp[city_id][len(clus_df.columns)] = 0\n",
    "    \n",
    "    # This is the current state of the clusters\n",
    "    clus_timeline[city_id][len(clus_df.columns)] = clus_df.columns\n",
    "\n",
    "    # Keep merging until only 2 clusters left\n",
    "    while len(clus_df.columns)>2:\n",
    "        # Find the best two clusters to merge\n",
    "        min_row = clus_df.min().idxmin()\n",
    "        if min_row!=min_row:\n",
    "            # It is possible our data has only unmergeable clusters remaining. It's not common though.\n",
    "            break\n",
    "        min_col = clus_df[min_row].idxmin()\n",
    "        if min_col!=min_col:\n",
    "            break\n",
    "\n",
    "        #make a new cluster from merging the best two\n",
    "        new_cluster_elems = clusters_id[city_id][min_row][0] + clusters_id[city_id][min_col][0]\n",
    "        obj_func = clus_df.at[min_row,min_col]\n",
    "        new_clus_id = clus_df.columns.max() + 1\n",
    "        clusters_id[city_id].update({new_clus_id:(new_cluster_elems, obj_func)})\n",
    "\n",
    "        # some debugging output\n",
    "        #print(f'min_row={min_row}, min_col={min_col}')\n",
    "        #print(f'{clusters_id[city_id][min_row][0]}+{clusters_id[city_id][min_col][0]}={new_cluster_elems} with {strain},{clus_df.min().min()}')\n",
    "\n",
    "        #drop the clusters that are merging from clus_df\n",
    "        clus_df = clus_df.drop(min_row, axis=0).drop(min_row, axis=1).drop(min_col, axis=0).drop(min_col, axis=1)\n",
    "\n",
    "        #add the new cluster to clus_df\n",
    "        new_row = [getClusNVar(new_cluster_elems+clusters_id[city_id][clus_id][0],D_imp_df[city_id],w_imp_df[city_id]) for clus_id in clus_df.columns.to_list()]\n",
    "        clus_df.loc[new_clus_id] = new_row\n",
    "        #now add the column, with the last element being the diagonal, so therefore NaN\n",
    "        new_row.append(np.NaN)\n",
    "        clus_df[new_clus_id] = new_row\n",
    "\n",
    "        #update our metrics\n",
    "        nClus_to_obj_func[city_id][len(clus_df.columns)] = np.sum([clusters_id[city_id][clus_id][1] for clus_id in clus_df.columns])\n",
    "        clus_timeline[city_id][len(clus_df.columns)] = clus_df.columns\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fc9c1-4f6e-491c-9478-9cade3f61629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for city_id in [0,1]:\n",
    "    print(f'City = {city_dict[city_id]}')\n",
    "    plt.figure()\n",
    "    plt.plot(nClus_to_obj_func[city_id].keys(),nClus_to_obj_func[city_id].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70925d8f-0c0f-4734-9542-e1937a2f553c",
   "metadata": {},
   "source": [
    "There's no really good way to say what the best number of clusters is, but the \"elbow method\" looks for where the above plots curve the most. That seems to be with about 10 clusters for Toronto, and 5 for Calgary. Let's see what these clusters look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a26b33-9841-40c0-a4ba-db5a62870fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getNamesInClusters(clus_ids, clusters, id_to_name):\n",
    "    cur_clusters = [clusters[clus_id][0] for clus_id in clus_ids]\n",
    "    return [[id_to_name[x] for x in clus] for clus in cur_clusters]\n",
    "\n",
    "# Toronto\n",
    "print(f'City = {city_dict[0]}')\n",
    "final_clusters = getNamesInClusters(clus_timeline[0][10],clusters_id[0],id_to_name[0])\n",
    "for i, clus in enumerate(final_clusters):\n",
    "    string = ''\n",
    "    for name in clus:\n",
    "        if name is clus[-1]:\n",
    "            string += name\n",
    "        else:\n",
    "            string += name + ', '\n",
    "    \n",
    "    print(i+1,string)\n",
    "    \n",
    "print()\n",
    "\n",
    "# Calgary\n",
    "print(f'City = {city_dict[1]}')\n",
    "final_clusters = getNamesInClusters(clus_timeline[1][5],clusters_id[1],id_to_name[1])\n",
    "for i, clus in enumerate(final_clusters):\n",
    "    string = ''\n",
    "    for name in clus:\n",
    "        if name is clus[-1]:\n",
    "            string += name\n",
    "        else:\n",
    "            string += name + ', '\n",
    "    \n",
    "    print(i+1,string)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23e9da-8a6d-416b-bbda-c0f42a402f35",
   "metadata": {},
   "source": [
    "So there we have it! I like to see Fords and Holydays generally placed together, and Tory with McKelvie also matches expectations. I'm a lot less knowledgeable about Calgary politics, but my brother who works at the city hall there says \"That's largely correct. I'd accept that sorting\". High praise indeed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e7550-f9ba-4ac4-9fad-443cb8f3b6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
